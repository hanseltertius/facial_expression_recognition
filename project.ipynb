{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e535cf5",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers datasets opencv-python gradio uniface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install old version of datasets to be able to use load_dataset()\n",
    "!pip install datasets==3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea2d6b",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc439f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from datasets import load_dataset\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from uniface import RetinaFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a70586",
   "metadata": {},
   "source": [
    "### Load Dataset from HuggingFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490b92ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 28709 | Test samples: 7178\n",
      "Class distribution in training set:  Counter({3: 7215, 4: 4965, 5: 4830, 2: 4097, 0: 3995, 6: 3171, 1: 436})\n",
      "angry: 3995\n",
      "disgust: 436\n",
      "fear: 4097\n",
      "happy: 7215\n",
      "neutral: 4965\n",
      "sad: 4830\n",
      "surprise: 3171\n",
      "Class distribution in test set:  Counter({3: 1774, 5: 1247, 4: 1233, 2: 1024, 0: 958, 6: 831, 1: 111})\n",
      "angry: 958\n",
      "disgust: 111\n",
      "fear: 1024\n",
      "happy: 1774\n",
      "neutral: 1233\n",
      "sad: 1247\n",
      "surprise: 831\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset from HuggingFace (FER2013)\n",
    "# This loads the FER2013 facial expression dataset from HuggingFace Datasets.\n",
    "dataset = load_dataset('Jeneral/fer-2013')\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']\n",
    "\n",
    "# Print the number of samples in each split\n",
    "print(f\"Train samples: {len(train_ds)} | Test samples: {len(test_ds)}\")\n",
    "\n",
    "# Check class distribution in training set\n",
    "labels_train = [sample['labels'] for sample in dataset['train']]  # Extract all labels from training set\n",
    "label_counts_train = Counter(labels_train)  # Count occurrences of each label\n",
    "print(\"Class distribution in training set: \", label_counts_train)\n",
    "class_names = dataset['train'].features['labels'].names  # Get class names from dataset metadata\n",
    "for idx, count in label_counts_train.items():\n",
    "    print(f\"{class_names[idx]}: {count}\")  # Print class name and count\n",
    "\n",
    "# Check class distribution in test set\n",
    "labels_test = [sample['labels'] for sample in dataset['test']]  # Extract all labels from test set\n",
    "label_counts_test = Counter(labels_test)  # Count occurrences of each label\n",
    "print(\"Class distribution in test set: \", label_counts_test)\n",
    "for idx, count in label_counts_test.items():\n",
    "    print(f\"{class_names[idx]}: {count}\")  # Print class name and count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad30876",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11de7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Define image transformations for training data augmentation and normalization.\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL image\n",
    "    transforms.RandomResizedCrop(128, scale=(0.9, 1.0)),  # Random crop and resize to 128x128\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.1),  # Randomly flip image vertically with 10% probability\n",
    "    transforms.RandomRotation(15),  # Randomly rotate image by up to 15 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),  # Random affine transformation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.07),  # Randomly change brightness, contrast, etc.\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)),  # Randomly erase part of image for regularization\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize image to match ImageNet stats\n",
    "])\n",
    "\n",
    "# Define image transformations for test/validation data (no augmentation, only resize and normalize)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL image\n",
    "    transforms.Resize((128, 128)),  # Resize image to 128x128\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize image\n",
    "])\n",
    "\n",
    "# Function to preprocess image bytes for training or testing\n",
    "def preprocess_image(img_bytes, train=True):\n",
    "    img_array = np.frombuffer(img_bytes, np.uint8)  # Convert bytes to numpy array\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)  # Decode image from array\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    if train:\n",
    "        img = transform_train(img)  # Apply training transformations\n",
    "    else:\n",
    "        img = transform_test(img)  # Apply test transformations\n",
    "    return img\n",
    "\n",
    "# Custom PyTorch Dataset for FER2013\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, split, train=True):\n",
    "        self.data = dataset[split]  # Load data split (train or test)\n",
    "        self.train = train  # Flag to indicate training or testing transformations\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Return number of samples\n",
    "    def __getitem__(self, idx):\n",
    "        img_bytes = self.data[idx]['img_bytes']  # Get image bytes\n",
    "        label = self.data[idx]['labels']  # Get label\n",
    "        img = preprocess_image(img_bytes, train=self.train)  # Preprocess image\n",
    "        return img, label  # Return image tensor and label\n",
    "\n",
    "# Create DataLoader for training and validation/test sets\n",
    "train_loader = DataLoader(FERDataset('train', train=True), batch_size=32, shuffle=True)  # Shuffle for training\n",
    "val_loader = DataLoader(FERDataset('test', train=False), batch_size=32)  # No shuffle for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24e23a",
   "metadata": {},
   "source": [
    "### Training Model - Definition Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f49360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model - Definition Part\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load VGG19 model pre-trained on ImageNet\n",
    "model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "# Replace the dropout layer in the classifier to increase regularization\n",
    "model.classifier[5] = nn.Dropout(0.5)  # Dropout set to 0.5 for regularization\n",
    "# Replace the final fully connected layer to match the number of classes in FER2013 (7 classes)\n",
    "model.classifier[6] = nn.Linear(4096, 7)  # FER2013 has 7 classes\n",
    "# Move model to the selected device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "# Get the count of each class from the training set\n",
    "class_counts = [label_counts_train[i] for i in range(len(class_names))]\n",
    "# Compute weights inversely proportional to class frequency\n",
    "class_weights = torch.tensor([1.0 / c for c in class_counts])\n",
    "# Normalize weights so their sum equals the number of classes\n",
    "class_weights = class_weights / class_weights.sum() * len(class_names)  # Normalize\n",
    "# Move class weights to the selected device\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Use class weights in the loss function to handle imbalance\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628a188",
   "metadata": {},
   "source": [
    "### Training Model - Execution Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbd2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model - Execution Part\n",
    "# Initialize optimizer (Adam) and learning rate scheduler (ReduceLROnPlateau)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Track best validation accuracy and early stopping parameters\n",
    "best_val_acc = 0\n",
    "epochs = 50\n",
    "early_stop_acc = 0.75\n",
    "counter = 0\n",
    "early_stop_patience = 10  # Number of epochs to wait for val_acc improvement\n",
    "no_improve_epochs = 0     # Counter for epochs with no val_acc improvement\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    train_loss = 0.0\n",
    "    train_iter = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for imgs, labels in train_iter:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(imgs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        _, preds = torch.max(outputs, 1)  # Get predicted class\n",
    "        correct += (preds == labels).sum().item()  # Count correct predictions\n",
    "        total += labels.size(0)  # Count total samples\n",
    "        train_loss += loss.item() * labels.size(0)  # Accumulate loss\n",
    "        train_iter.set_postfix({'loss': loss.item()})  # Show current loss in progress bar\n",
    "    train_acc = correct / total  # Calculate training accuracy\n",
    "    avg_train_loss = train_loss / total  # Calculate average training loss\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(val_loader, desc='Validation', leave=False)\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for imgs, labels in val_iter:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(imgs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            _, preds = torch.max(outputs, 1)  # Get predicted class\n",
    "            correct += (preds == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)  # Count total samples\n",
    "            val_loss += loss.item() * labels.size(0)  # Accumulate loss\n",
    "            val_iter.set_postfix({'loss': loss.item()})  # Show current loss in progress bar\n",
    "    val_acc = correct / total  # Calculate validation accuracy\n",
    "    avg_val_loss = val_loss / total  # Calculate average validation loss\n",
    "\n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Print training and validation metrics for this epoch\n",
    "    print(f\"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    print(f\"Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping: stop when both train and val accuracy reach threshold\n",
    "    if train_acc >= early_stop_acc and val_acc >= early_stop_acc:\n",
    "        print(\"Early stopping: both train and validation accuracy reached threshold.\")\n",
    "        torch.save(model.state_dict(), \"best_fer32_model.pth\")  # Save model\n",
    "        break\n",
    "\n",
    "    # Early stopping: stop if val_acc does not improve for early_stop_patience epochs\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc  # Update best validation accuracy\n",
    "        torch.save(model.state_dict(), \"best_fer32_model.pth\")  # Save model\n",
    "        no_improve_epochs = 0  # Reset no improvement counter\n",
    "    else:\n",
    "        no_improve_epochs += 1  # Increment no improvement counter\n",
    "        print(f\"No improvement in val_acc for {no_improve_epochs} epoch(s)\")\n",
    "        if no_improve_epochs >= early_stop_patience:\n",
    "            print(f\"Early stopping: validation accuracy did not improve for {early_stop_patience} consecutive epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d8ccd",
   "metadata": {},
   "source": [
    "### Gradio Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7824bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Interface\n",
    "# Load RetinaFace model for face detection with custom parameters\n",
    "retinaface = RetinaFace(\n",
    "    model=\"retinaface_r34\",  # Use ResNet34-based RetinaFace model\n",
    "    conf_thresh=0.3,         # Confidence threshold for face detection\n",
    "    nms_thresh=0.3,          # Non-maximum suppression threshold (lower = stricter)\n",
    "    input_size=1024,         # Input size for face detector (higher = more accurate, slower)\n",
    "    dynamic_size=True        # Allow dynamic input size for different images\n",
    " )\n",
    "\n",
    "# Load emotion recognition model\n",
    "# Load the best saved model weights for emotion recognition\n",
    "model.load_state_dict(torch.load(\"best_fer32_model.pth\", map_location=device))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "def detect_emotion(image):\n",
    "    # Detect faces in the input image using RetinaFace\n",
    "    faces = retinaface.detect(image)\n",
    "    if faces is None or len(faces[0]) == 0:\n",
    "        return image, {\"error\": \"No face detected\"}  # No face detected, return error message\n",
    "\n",
    "    results = []\n",
    "    boxes = faces[0]  # bounding boxes: [x1, y1, x2, y2, score]\n",
    "    annotated_img = image.copy()  # Copy image for annotation\n",
    "    for box in boxes:\n",
    "        # Extract bounding box coordinates and score\n",
    "        x1, y1, x2, y2, _ = int(box[0]), int(box[1]), int(box[2]), int(box[3]), float(box[4])\n",
    "        face_img = image[y1:y2, x1:x2]  # Crop face from image\n",
    "        face_img_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        face_img_resized = cv2.resize(face_img_rgb, (128, 128))  # Resize to model input size\n",
    "        face_tensor = transform_test(face_img_resized).unsqueeze(0).to(device)  # Preprocess and add batch dimension\n",
    "        with torch.no_grad():  # Disable gradients for inference\n",
    "            outputs = model(face_tensor)  # Get model predictions\n",
    "            probs = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "            conf, pred = torch.max(probs, 1)  # Get highest probability and predicted class\n",
    "            emotion = class_names[pred.item()]  # Get emotion label\n",
    "            confidence = round(conf.item() * 100, 2)  # Convert confidence to percentage\n",
    "        # Draw bounding box and label on annotated image\n",
    "        cv2.rectangle(annotated_img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw rectangle around face\n",
    "        cv2.putText(annotated_img, f\"{emotion} ({confidence}%)\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)  # Draw label above face\n",
    "        results.append({\n",
    "            \"emotion\": emotion,\n",
    "            \"confidence\": confidence,\n",
    "            \"bbox\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2}\n",
    "        })  # Add result for this face\n",
    "    return annotated_img, {\"result\": results}  # Return annotated image and results as 'result' array\n",
    "\n",
    "# Create Gradio interface for emotion detection\n",
    "iface = gr.Interface(\n",
    "    fn=detect_emotion,  # Function to run for each input\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload Image\"),  # Input: image upload\n",
    "    outputs=[gr.Image(type=\"numpy\", label=\"Annotated Image\"), gr.JSON(label=\"Detection Results\")],  # Output: annotated image and JSON results\n",
    "    title=\"Facial Emotion Recognition with RetinaFace & VGG19 Pretrained Model\",\n",
    "    description=\"Upload an image. The app will detect all human faces using RetinaFace powered by uniface library, which predict the emotion and confidence for each face using VGG19 model trained on FER2013 Dataset from HuggingFace. Returns annotated image and JSON with emotion, confidence, and bounding box coordinates.\"\n",
    " )\n",
    "\n",
    "iface.launch()  # Launch Gradio app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
