{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e535cf5",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac7a49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio gradio opencv-python datasets transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea2d6b",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc439f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import gradio as gr\n",
    "import cv2\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a70586",
   "metadata": {},
   "source": [
    "### Load Dataset from HuggingFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b92ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Dataset from HuggingFace (FER2013)\n",
    "dataset = load_dataset('Jeneral/fer-2013')\n",
    "\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']\n",
    "print(f\"Train samples: {len(train_ds)} | Test samples: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad30876",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de7a1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert 'img_bytes' to PIL image, then to OpenCV for optional processing\n",
    "        img_bytes = self.dataset[idx]['img_bytes']\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "        # OpenCV preprocessing: histogram equalization (enabled)\n",
    "        img_cv = np.array(img)\n",
    "        img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n",
    "        img_yuv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2YUV)\n",
    "        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "        img_eq = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "        img = Image.fromarray(img_eq)\n",
    "        label = self.dataset[idx]['labels']\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Data augmentation and normalization (32x32, only defined once)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # reduced\n",
    "    transforms.RandomResizedCrop(32, scale=(0.9, 1.0)),  # less aggressive\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = FERDataset(train_ds, transform=train_transform)\n",
    "test_data = FERDataset(test_ds, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24e23a",
   "metadata": {},
   "source": [
    "### Training Model - Definition Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f49360",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training Model - Definition Part (ResNet18, Adam optimizer, label smoothing, class weighting, early stopping at 80% accuracy, model checkpointing, model saving)\n",
    "num_classes = len(set(train_ds['labels']))\n",
    "# Use ResNet18 for 32x32 images\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# Adjust first conv layer for 32x32 input\n",
    "model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = torch.nn.Identity()\n",
    "# Unfreeze all layers for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# Add dropout to the classifier head\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "model = model.to(device)\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "label_counts = Counter([sample['labels'] for sample in train_ds])\n",
    "total = sum(label_counts.values())\n",
    "class_weights = [total / label_counts[i] for i in range(num_classes)]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Label smoothing loss\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, pred, target):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        logprobs = torch.nn.functional.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.05)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628a188",
   "metadata": {},
   "source": [
    "### Training Model - Execution Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbd2c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training Model - Execution Part\n",
    "\n",
    "# Reduce max epochs for faster experimentation\n",
    "epochs = 20\n",
    "best_val_acc = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Use torch.amp for mixed precision (PyTorch 2.0+)\n",
    "scaler = torch.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_batches = len(train_loader)\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f'Training Epoch {epoch+1}', total=total_batches)):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, 1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/(batch_idx+1):.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Validation on a larger test subset\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            if scaler:\n",
    "                with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                    outputs = model(images)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "            preds = torch.argmax(outputs, 1).detach().cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(labels.detach().cpu().numpy())\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Step scheduler based on validation accuracy\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Save best model based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Early stopping if both train and validation accuracy >= 0.80\n",
    "    if acc >= 0.80 and val_acc >= 0.80:\n",
    "        print(f\"Early stopping: Train and Validation accuracy >= 80% at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "# Load best model weights after training\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(f\"Best Validation Accuracy Achieved: {best_val_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'best_fer32_model.pth')\n",
    "print('Best model saved as best_fer32_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d8ccd",
   "metadata": {},
   "source": [
    "### Gradio Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7824bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gradio Interface Part\n",
    "def load_model(weights_path='best_fer32_model.pth'):\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = torch.nn.Identity()\n",
    "    num_classes = len(train_ds.features['labels'].names)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_infer = load_model()\n",
    "\n",
    "def predict(img):\n",
    "    with torch.no_grad():\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        img_t = test_transform(img).unsqueeze(0).to(device)\n",
    "        output = model_infer(img_t)\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        pred = int(np.argmax(probs))\n",
    "        confidence = float(probs[pred]) * 100\n",
    "        label_map = train_ds.features['labels'].names\n",
    "        return {\n",
    "            'Expression': label_map[pred],\n",
    "            'Confidence': f'{confidence:.2f}%'\n",
    "        }\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"numpy\"),\n",
    "    outputs=gr.JSON(),\n",
    "    title=\"Facial Expression Recognition (Transfer Learning)\",\n",
    "    description=\"Upload a face image. The model will predict the expression and show the confidence.\")\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
